---
layout: post
title: Algorithms for Data Science
date: 2022-01-02 11:18 +0800
tags: [Programming]
toc:  true
---

<!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-TG0XJZG53F"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-TG0XJZG53F');
  </script>

  <style TYPE="text/css">code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}</style><script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [['$','$'], ['\\(','\\)']],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
      }});
  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i = 0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }});
  </script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full"></script>  

## Week 1 Random Sampling

### Discrete Probability Distribution<br/>

A discrete probability distribution is a probability distribution that can take on a countable number of values. In the case where the range of values is countably infinite, these values have to decline to zero fast enough for the probabilities to add up to 1. For example, if $P(X=n)  =  \frac{1}{2^n} $ for n = 1,2,..., the sum of probabilities would be 1/2 + 1/4 + 1/8 + ... = 1

Examples include the Poisson distribution, the Bernoulli distribution, and the binomial distribution.

When a sample (a set of observations) is drawn from a larger population, the sample points have an empirical distribution that is discrete, and which provides information about the population distribution.

### Probability Mass Function (PMF)

A probability mass function (PMF) is a function that gives the probability that a discrete random variable is exactly equal to some value. Sometimes it is also known as the discrete density function.

![](https://joy3luo.github.io/mathnotes/pics/MSML606/distrib.png)

### Cumulative Distribution Function (CDF)

A cumulative distribution function (CDF) of a random variable  X  , or just distribution function of  X , evaluated at  x , is the probability that  X  will take a value less than or equal to  x .

![](https://joy3luo.github.io/mathnotes/pics/MSML606/distribution.png)

### Inverse distribution function (quantile function)

The quantile function, associated with a probability distribution of a random variable, specifies the value of the random variable such that the probability of the variable being less than or equal to that value equals the given probability. It is also called the percent-point function or inverse cumulative distribution function.

![](https://joy3luo.github.io/mathnotes/pics/MSML606/function.png)

#### Example

Here is an intuitive explanation:

P(Weak)=0.2 , P(Standard)=0.7, P(Strong) = 0.1

![](https://joy3luo.github.io/mathnotes/pics/MSML606/1.jfif)

### Continuous Probability Distribution

A continuous probability distribution is a probability distribution whose support is an uncountable set, such as an interval in the real line. They are uniquely characterized by a cumulative distribution function that can be used to calculate the probability for each subset of the support. There are many examples of continuous probability distributions: normal, uniform, chi-squared, and others.

### Probability Density Function (PDF)

A probability density function (PDF), or density of a continuous random variable, is a function whose value at any given sample (or point) in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the random variable would equal that sample. (Wikipedia)

![](https://joy3luo.github.io/mathnotes/pics/MSML606/2.png)

### Inverse Transform Sampling

Inverse transform sampling is a basic method for pseudo-random number sampling, i.e., for generating sample numbers at random from any probability distribution given its cumulative distribution function.

#### Example: Bernoulli Distribution

![](https://joy3luo.github.io/mathnotes/pics/MSML606/3.png)

#### Theorem

Reference: Machine Learning - A Probabilistic Perspective, Kevin P. Murphy, Page 818

If  $U∼U(0,1)$  is a uniform random variable, then  $F^{−1}(U)∼F$

Proof:

$Pr(F^{-1}(U) \le x) = Pr(U \le F(x))$, applying F to both sides

$=F(x)$, because $Pr(U \le y)=y$

The first line follows since F is a monotonic function and the second line follows since U is uniform on the unit interval.

#### Example

The probability density function (pdf) of an exponential distribution is

${f(x;\lambda )={\begin{cases}\lambda e^{-\lambda x}&x\geq 0,\\0&x<0.\end{cases}}}$

Here λ > 0 is the parameter of the distribution

The cumulative distribution function is given by

${\displaystyle F(x;\lambda )={\begin{cases}1-e^{-\lambda x}&x\geq 0,\\0&x<0.\end{cases}}}$

The inverse cumulative function is

${\displaystyle F^{-1}(p)={\begin{cases} {\tfrac {-ln(1-p)}{\lambda}} &x\geq 0,\\0&x<0.\end{cases}}}$


```py
import numpy as np

lambda_1 = 2                                 
U=np.random.random(10000)            
X=-np.log(1-U)/lambda_1
np.mean(X) #0.4937494667083374
```

random numbers from a Bernoulli distribution (binary outcome) e.g., tossing a  coin heads/tails 10 times

```py
import numpy as np

 # 0---p---1

def bern(p):
  r = np.random.uniform(0,1)
  if r < p:
    return 1
  else:
    return 0

size = 10
for i in range(size):
  coin = bern(0.5)  # fair coin
  print(coin)

-----
0
1
0
0
0
0
1
0
0
0
```

off-the-shelf use of a library

```py
from scipy.stats import bernoulli
size = 10
for i in range(size):
  coin = bernoulli.rvs(0.5)  # fair coin
  print(coin)
-----
1
1
1
0
0
0
1
0
1
0
```

#### Example: Binomial Distribution

Binomial distribution: binomial random variable counts the number of heads or positives or successes x in n repeated trials of a binomial experiment

simulation of the number of heads and tails by tossing a fair coin 10 times
```py
def bern(p):
  r = np.random.uniform(0,1)
  if r < p:
    return 1
  else:
    return 0                     

size = 10
x = [0,1]
y = [0,0]

for i in range(size):
  index = bern(0.5)
  y[index] +=1

print(y) #[6, 4]
```

![](https://joy3luo.github.io/mathnotes/pics/MSML606/binomial.png)

Bernoulli as a special case of Binomial  

Bernoulli distribution is the discrete probability distribution of a random variable which takes the value 1 with probability $p$ and the value 0 with probability $q=1-p$.

```py                                                             
from scipy.stats import binom

size = 10
for i in range(size):
  coin = binom.rvs(1,0.5, size=1)  # fair coin       
  print(coin)
-----
[1]
[0]
[0]
[0]
[0]
[1]
[1]
[0]
[0]
[1]
```

#### Example: Categorical Distribution

A categorical distribution (also called a generalized Bernoulli distribution, multinoulli distribution) is a discrete probability distribution that describes the possible results of a random variable that can take on one of K possible categories, with the probability of each category separately specified.(Wikipedia)

Example: random outcomes of a die tossed 10 times
0-------p1----p1+p2------p1+p2+p3-------p1+p2+p3+p4----p1+p2+p3+p4+p5------1
```py
def categ(p1,p2,p3,p4,p5,p6):
  r = np.random.uniform(0,1)
  if r < p1:
    return 1
  elif r < p1 + p2:
    return 2
  elif r < p1 + p2 + p3:
    return 3
  elif r < p1 + p2 + p3 + p4:
    return 4
  elif r < p1 + p2 + p3 + p4 + p5:
    return 5
  else: return 6

size = 10
for i in range(size):
  dice = categ(1/6,1/6, 1/6, 1/6, 1/6, 1/6)  # fair die
  print(dice)
------
2
6
2
1
4
1
2
2
4
3
```

#### Multinomial Distribution

Multinomial distribution is the generalization of the binomial distribution when the categorical variable has more than two outcomes

Probability of multinomial distribution

# $\frac{n!}{x_1!\cdots x_k!} p_1^{x_1} \cdots p_k^{x_k}$

Example: Simulation of the number of times each side of a fair die shows up if the die is tossed 10 times
```py
y = [0,0,0,0,0,0,0] # initialize count to zero
size = 10

for i in range(size):
  dice = categ(1/6,1/6, 1/6, 1/6, 1/6, 1/6)
  y[dice] += 1

print(y[1:])
-----
[0, 4, 2, 1, 1, 2]
```

#### Beta Distribution

The beta distribution is a family of continuous probability distributions defined on the interval [0, 1] parameterized by two positive shape parameters, denoted by α and β, that appear as exponents of the random variable and control the shape of the distribution. The generalization to multiple variables is called a Dirichlet distribution.

In Bayesian inference, the beta distribution is the conjugate prior probability distribution for the Bernoulli, binomial, negative binomial and geometric distributions. The beta distribution is a suitable model for the random behavior of percentages and proportions.

The probability density function (pdf) of the beta distribution, for 0 ≤ x ≤ 1, and shape parameters α, β > 0, is a power function of the variable x and of its reflection (1 − x) as follows:

${\displaystyle {\begin{aligned}f(x;\alpha ,\beta )&=\mathrm {constant} \cdot x^{\alpha -1}(1-x)^{\beta -1}\\[3pt]&={\frac {x^{\alpha -1}(1-x)^{\beta -1}}{\displaystyle \int _{0}^{1}u^{\alpha -1}(1-u)^{\beta -1}\,du}}\\[6pt]&={\frac {\Gamma (\alpha +\beta )}{\Gamma (\alpha )\Gamma (\beta )}}\,x^{\alpha -1}(1-x)^{\beta -1}\\[6pt]&={\frac {1}{\mathrm {B} (\alpha ,\beta )}}x^{\alpha -1}(1-x)^{\beta -1}\end{aligned}}}$

where Γ(z) is the gamma function.

![image](https://upload.wikimedia.org/wikipedia/commons/7/78/PDF_of_the_Beta_distribution.gif)

##### Dirichlet distribution

The Dirichlet distribution  ${\displaystyle \operatorname {Dir} ({\boldsymbol {\alpha }}})$, is a family of continuous multivariate probability distributions parameterized by a vector ${\displaystyle {\boldsymbol {\alpha }}}$ of positive reals.

The Dirichlet distribution of order K ≥ 2 with parameters α1, ..., αK > 0 has a probability density function  given by

${\displaystyle f\left(x_{1},\ldots ,x_{K};\alpha _{1},\ldots ,\alpha _{K}\right)={\frac {1}{\mathrm {B} ({\boldsymbol {\alpha }})}}\prod _{i=1}^{K}x_{i}^{\alpha _{i}-1}}$
where ${\displaystyle \{x_{k}\}_{k=1}^{k=K}}$ belong to the standard ${\displaystyle K-1}$ simplex, or in other words: ${\displaystyle \sum _{i=1}^{K}x_{i}=}1$  and  $x_{i}≥0$ for all $i∈${1,…,K}
The normalizing constant is the multivariate beta function, which can be expressed in terms of the gamma function:

${\displaystyle \mathrm {B} ({\boldsymbol {\alpha }})={\frac {\prod _{i=1}^{K}\Gamma (\alpha _{i})}{\Gamma \left(\sum _{i=1}^{K}\alpha _{i}\right)}},\qquad {\boldsymbol {\alpha }}=(\alpha _{1},\ldots ,\alpha _{K}).}$

![image](https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Dirichlet.pdf/page1-1500px-Dirichlet.pdf.jpg)


```py

```


```py

```


```py

```


```py

```


```py

```


```py

```


```py

```


```py

```


```py

```
